# An√°lise de Algoritmos Paralelos e Seriais
Universidade Federal do Rio Grande do Norte ([UFRN](http://http://www.ufrn.br)), 2020.

#### An√°lise por:
- [Oziel Alves](https://github.com/ozielalves/)

## Sum√°rio
+ [Introdu√ß√£o](#introdu√ß√£o)
  + [Objetivos](#objetivos)
  + [Depend√™ncias](#depend√™ncias)
    + [G++ Compiler](#g-compiler)
    + [MPI](#mpi---message-passing-interface)
  + [Compila√ß√£o e Execu√ß√£o](#compila√ß√£o-e-execu√ß√£o)
    + [Arquivo com Resultados](#arquivo-com-resultados)
    + [Condi√ß√µes de Testes](#condi√ß√µes-de-testes)
      + [Informa√ß√µes sobre a m√°quina utilizada](#informa√ß√µes-sobre-a-m√°quina-utilizada)
  + [Apresenta√ß√£o dos Algoritmos](#apresenta√ß√£o-dos-algoritmos)
    + [C√°lculo do Pi](#c√°lculo-do-pi---m√©todo-de-monte-carlo)
      + [Serial](#serial)
      + [Paralelo](#paralelo
    + [C√°lculo da Integral - Regra do Trap√©zio](#c√°lculo-da-integral---regra-do-trap√©zio)
      + [Serial](#serial)
      + [Paralelo](#paralelo)
+ [Desenvolvimento](#desenvolvimento)      
  + [Resultados](#resultados)
    + [C√°lculo do Pi](#c√°lculo-do-pi)
      + [Corretude](#corretude)
      + [Gr√°ficos](#gr√°ficos)
      + [Serial e Paralelo - Tempo x Tamanho do Problema](#serial-e-paralelo---tempo-x-tamanho-do-problema)
      + [Paralelo - Tempo x Cores](#paralelo---tempo-x-cores)
      + [An√°lise de Speedup](#an√°lise-de-speedup)
      + [An√°lise de Efici√™ncia](#an√°lise-de-efici√™ncia)
    + [C√°lculo da Integral](#c√°lculo-da-integral)
      + [Corretude](#corretude)
      + [Gr√°ficos](#gr√°ficos)
      + [Serial e Paralelo - Tempo x Tamanho do Problema](#serial-e-paralelo---tempo-x-tamanho-do-problema)
      + [Paralelo - Tempo x Cores](#execu√ß√£o-do-problema---tempo-x-cores)
      + [An√°lise de Speedup](#an√°lise-de-speedup)
      + [An√°lise de Efici√™ncia](#an√°lise-de-efici√™ncia)
+ [Conclus√£o](#conclus√£o)
  + [Considera√ß√µes Finais](#considera√ß√µes-finais)
  + [Softwares utilizados](#softwares-utilizados)

## Introdu√ß√£o

### Objetivos
Analisar e avaliar o comportamento, efici√™ncia e speedup dos algoritmos em rela√ß√£o ao seu tempo de execu√ß√£o, tamanho do problema analisado e resultados obtidos. Os cen√°rios ir√£o simular a execu√ß√£o dos algoritmos para 2, 4 e 8 cores, no caso dos algoritmos paralelos, com alguns tamanhos de problema definidos empiricamente, sendo o menor tamanho estabelecido com o objetivo de atingir o tempo m√≠nimo de execu√ß√£o determinado pela refer√™ncia da An√°lise (30 segundos).

### Depend√™ncias
#### G++ Compiler
√â necess√°rio para a compila√ß√£o dos programam, visto que s√£o feitos em c++.
```bash
# Instala√ß√£o no Ubuntu 20.04 LTS:
sudo apt-get install g++
```
<br>
<br>
<br>

#### MPI - Message Passing Interface
√â necess√°rio para a compila√ß√£o e execu√ß√£o dos c√≥digos paralelos.
```bash
# Instala√ß√£o no Ubuntu 20.04 LTS:
sudo apt-get install -y mpi 
```
### Compila√ß√£o e Execu√ß√£o
Instaladas as depend√™ncias, basta executar o shellcript determinado para a devida bateria de execu√ß√µes na raiz do reposit√≥rio:<br>
Ser√£o realizadas **5 execu√ß√µes** com **4 tamanhos de problema** , em **3 quantidades de cores** (2, 4 e 8).
```bash
# Para o algor√≠timo que calcula o pi de forma serial
./pi_serial_start.sh
```
```bash
# Para o algor√≠timo que calcula o pi de forma paralela
./pi_paralelo_start.sh
```
```bash
# Para o algor√≠timo que calcula o pi de forma serial
./trapezio_serial_start.sh
```
```bash
# Para o algor√≠timo que calcula o pi de forma paralela
./trapezio_paralelo_start.sh
```

**Obs.:** Caso seja necess√°rio conceder permiss√£o m√°xima para os scripts, execute `chmod 777 [NOME DO SCRIPT].sh`.

### Arquivo com Resultados 
Ap√≥s o termino das execu√ß√µes do script √© poss√≠vel ter acesso aos arquivos `.txt` na pasta `pi` ou `trapezio`, de acordo com o script executado. Os dados coletados foram utilizados para realiza√ß√£o desta an√°lise.

### Condi√ß√µes de Testes
#### Informa√ß√µes sobre a m√°quina utilizada
+ **Dell Inspiron 14-inc 7460**

+ **Processador**: Intel Core i7 7500U (at√© 3.5 GHz) Dual Core Cache 4M. (FSB)4 GT/s OPI (Integra HyperThreading para trabalhar com at√© 4 threads de uma vez)

+ **N√∫mero de Cores/Threads**: 2/4

+ **Mem√≥ria**: 8 GB tipo DDR4 ‚Äì 2133MHz

+ **Sistema**: Ubuntu 20.04.1 LTS

## Desenvolvimento

### Apresenta√ß√£o dos Algoritmos

#### C√°lculo do Pi - M√©todo de Monte Carlo
O algoritmo √© baseado no m√©todo de Monte Carlo para estimar o valor de **`ùúã`**. Ele depende de amostragem independente e aleat√≥ria repetida, e funciona bem com sistemas paralelos e distribu√≠dos, pois o trabalho pode ser dividido entre v√°rios processos. Sua ideia principal √© simular um grande n√∫mero de realiza√ß√µes de um evento estat√≠stico. Neste sentido, o uso de multiplos processadores permite a realiza√ß√£o de um n√∫mero fixo de eventos por processador, o que aumenta o n√∫mero total de eventos simulados.<br> 
No c√°lculo de Pi, em espec√≠fico, o algoritmo implementado tem como base a gera√ß√£o de diversos pontos, cujas coordenadas s√£o n√∫meros aleat√≥rios com fun√ß√£o de desnsidade de probabilildade constante em um intervalo indo de 0 a 1. Assim, a probabilidadede que os pontos estejam dentro do quadrado definido pelo produto cartesiano [0,1]x[0,1] √© unit√°ria. Se, de todos os pontos gerados, contarmos aqueles cuja norma euclidiana √© menor ou igual a 1, √© poss√≠vel encontrar a probabilidade de que um ponto esteja dentro do quarto de c√≠rculo centrado na origem e de raio 1, que √© proporcional a sua √°rea. Com isso, e sabendo a √°rea de 1/4 de c√≠rculo, basta uma manipula√ß√£o alg√©brica para encontrar o valor de pi aproximado. Assim:

```bash
   pi = 4 * (pontos_dentro_do_c√≠rculo)/(pontos_totais)
```

#### Serial
Dado um n√∫mero de pontos a serem definidos, que chamaremos de `termos`, a seguinte sub-rotina √© implementada: 

1. √â setado o valor `acertos` = 0.0.

2. `termos` determinar√° a quatidade de pontos `x` e `y` a serem definidos randomicamente com seed fixa = 42, dentro do intervalo de 0.0 a 1.0.

3. Caso `( x * x + y * y )` seja menor que 1.0, `acertos` √© acrescido em 1 unidade.

4. Ao termino do la√ßo, para conclus√£o do m√©todo de Monte Carlo,  √© retornado `acertos` multiplicado por 4 e dividido por `termos`.

A implementa√ß√£o da fun√ß√£o `calcPi` √© apresentada abaixo:
```bash
double calcPi(int termos)
{
    # Gerador Mersene twist, SEED: 42
    mt19937 mt(42);
    
    # Numero real pseudo-aleatorio
    uniform_real_distribution<double> linear_r(0.f, 1.f);

    int acertos = 0;
    for (int i = 0; i < termos; i++)
    {

        double x = linear_r(mt);
        double y = linear_r(mt);
        
        if (x * x + y * y < 1.0)
        {
            acertos++;
        }
    }
    return (double)(4.0 * acertos / termos);
}
```

#### Paralelo
Ainda chamando o numero total de pontos a serem definidos como `termos`, a seguinte sub-rotina √© implementada:  

1. O tamanho do problema, `termos` √© lido por por linha de comando.

2. √â iniciada a comunica√ß√£o paralela.

3. `termos_local` recebe `termos` dividido pela quantidade de processos.

4.  `termos_local` √© passado como parametro para o c√°culo parcial dos acertos, usando a fun√ß√£o j√° apresentada, `calcPi`, e armazenado em cada processo como `acertos_parc`.

5. Ao termino da execu√ß√£o de cada processo , `acertos_parc` √© somado a `acertos`.

6. Quando todos os processos finalizam a contagem de acertos, todos os acertos parciais s√£o somados a `acertos`, ent√£o, √© fechada a comunica√ß√£o MPI e impresso o valor do resultado final multiplicado por 4 e dividido por `termos`.

**Obs.:** Vale salientar que, por escolha particular, a multiplica√ß√£o e divis√£o no n√∫mero de acertos foi realizada apenas na impress√£o do resultado. Diferentemente do que acontece naturalmente na fun√ß√£o `calcPi`, no c√≥digo paralelo √© retornado apenas a quantidade de acertos.

A implementa√ß√£o do Paralelismo √© apresentada abaixo:
```bash
int main(int argc, char **argv)
{
    struct timeval start, stop;
    gettimeofday(&start, 0);

    int my_rank;
    int p;
    int termos = atoll(argv[1]);
    int termos_local;
    int inicial_local;
    double acertos_parc;
    double acertos;

    MPI_Init(&argc, &argv);

    # Rank do meu processo
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

    # Descobre quantos processos estao em uso
    MPI_Comm_size(MPI_COMM_WORLD, &p);

    # Divisao interna
    termos_local = termos / p;

    # Bloqueia o processo at√© todos chegarem nesse ponto
    MPI_Barrier(MPI_COMM_WORLD);

    acertos_parc = calcPi(termos_local);

    # Soma o numero de acertos por cada processo
    MPI_Reduce(&acertos_parc, &acertos, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
    if (my_rank == 0)
    {
        gettimeofday(&stop, 0);

        FILE *fp;
        char outputFilename[] = "./pi/tempo_mpi_pi.txt";

        fp = fopen(outputFilename, "a");
        if (fp == NULL)
        {
            fprintf(stderr, "Nao foi possivel abrir o arquivo %s!\n", outputFilename);
            exit(1);
        }

        fprintf(fp, "\tTempo: %1.2e \tResultado: %f\n",
                ((double)(stop.tv_usec - start.tv_usec) / 1000000 + (double)(stop.tv_sec - start.tv_sec)),
                (double)4 * acertos / termos);

        fclose(fp);
    }
    else
    { /* Nothing */ }

    MPI_Finalize();
}
```

### C√°lculo da Integral - Regra do Trap√©zio
A regra do trap√©zio √© um m√©todo para aproximar a integral de uma fun√ß√£o, `y = f (x)`, usando trap√©zios para calcular a √°rea. O processo √© simples. Sejam `xa` e `xb` os pontos que limitam o intervalo para ser feito o c√°lculo da integral, e seja `n` o n√∫mero de sub-intervalos de `xa` at√© `xb`. Para cada sub-intervalo, a fun√ß√£o √© aproximada com uma linha reta entre os valores da fun√ß√£o em ambas as extremidades do sub-intervalo. Cada sub-intervalo agora √© um mini-trap√©zio. Por √∫ltimo, a √°rea de cada mini-trap√©zio √© calculada e todas as √°reas s√£o somadas para obter uma aproxima√ß√£o da integral da fun√ß√£o `f` de `xa` a `xb`. Assim:

![Alt Formula - Regra do Trap√©zio](./data/trapezio_graphs/Formula_trapezio.png)

#### Serial
Dado um `n`, tal que representa o n√∫mero de mini-trap√©zios a dividir o intervalo, a seguinte sub-rotina √© implementada: 

1. √â setado o intervalo `xa` = 0.0 `xb`= 30.0 na fun√ß√£o `main`.

2. √â realizada ent√£o a chamada da fun√ß√£o `trapezioIntegral` passando como par√¢metros `xa`, `xb` e `n`.

3. O valor da base de cada mini-trap√©zio no intervalo √© definido pela substra√ß√£o de `xb` por `xa` divido por `n`, chamaremos de `inc`.
  
4. O valor da `area_total` recebe inicialmente `(f(xa) + f(xb)) / 2`.

5. Sendo `x_i` o passo do x de um sub-intervalo a outro, em um la√ßo de `x_1` at√© `x_n-1` os valores de `f(x_i)` s√£o acrescidos a `area_total`.

6. Ao termino do la√ßo, para conclus√£o do c√°culo da integral pela regra do trap√©zio, `area_total` √© multiplicada por `inc` e retornada pela fun√ß√£o.

A implementa√ß√£o da fun√ß√£o `trapezioIntegral` √© apresentada abaixo:
```bash
double trapezioIntegral(double xa, double xb, long long int n)
{
    double x_i;             # Passo do X
    double area_total = 0.; # Soma das areas
    double inc;             # Incremento

    inc = (xb - xa) / n;
    area_total = (f(xa) + f(xb)) / 2;

    for (long long int i = 1; i < n; i++)
    {
        x_i = xa + i * inc;
        area_total += f(x_i);
    }

    area_total = inc * area_total;

    return area_total;
}
```

#### Paralelo
Para implementa√ß√£o da regrado do trap√©zio de modo paralelo, √© preciso primeiro identificar as tarefas necess√°rias e mapear as tarefas para todos os processos. Sendo assim, √© preciso: 
1. Encontrar a √°rea de muitos trap√©zios individuais, o retorno destas √°reas parciais ser√£o atribuidos localmente √† `area_relativa`.
2. Somar essas √°reas, a soma total ser√° atribuida √† `area_total`.

Intuitivamente, conforme aumentamos o n√∫mero de trap√©zios, receberemos uma previs√£o mais precisa da integral calculada. Assim, estaremos usando mais trap√©zios do que cores neste problema, √© preciso dividir os c√°lculos para calcular as √°reas dos mini-trap√©zios. O procedimento ser√° realizado atribuindo a cada processo um subintervalo que cont√©m o n√∫mero de trap√©zios, obtidos a partir do c√°lculo do n√∫mero total de trap√©zios `n`, dividido pelo n√∫mero de processos. Isso pressup√µe que o n√∫mero total de trap√©zios √© igualmente divis√≠vel pelo n√∫mero de processos. Cada processo aplicar√° a regra do trap√©zio ao seu subintervalo. Por √∫ltimo, o processo mestre soma as estimativas.

<br>
Dado um `n`, tal que representa o n√∫mero de trap√©zios a dividir o intervalo, a seguinte sub-rotina √© implementada:  

1. √â iniciada a comunica√ß√£o paralela.

2. `n` √© passado como argumento para a fun√ß√£o auxiliar `setSize` junto ao rank do processo, `my_rank`, para destribuir o tamanho do problema para todos os processos usando `MPI_Bcast`.

3. O incremento √© calculado pela divis√£o por `n` do resultado da subtra√ß√£o de `xb` por `xa`.

4. O n√∫mero de trap√©zios a ser calculados por cada processo, `local_n`, √© definido atrav√©s da divis√£o de `n` pelo n√∫meor de processos, `p`.

5. Cada processo calcula a `area_relativa` ao seu intervalo.

6. Quando todos os processos finalizam o c√°culo da integral de seus respectivos intervalos, o valor de cada integral parcial √© somado a `area_total`, ent√£o,  √© fechada a comunica√ß√£o MPI.

A implementa√ß√£o do Paralelismo √© apresentada abaixo:
```bash
int main(int argc, char **argv)
{
    struct timeval start, stop; # Intervalo de tempo calculado ao fim
    gettimeofday(&start, 0);

    int my_rank = 0;           # Rank do meu processo
    int p = 0;                 # Numero de processos
    const double xa = 0.;      # X In√≠cio da figura
    const double xb = 30.;     # X Fim da figura
    double n = 0.;             # Numero de mini trapezios
    double inc = 0.;           # Incremento (Base do Trapezio)
    double local_a = 0.;       # X In√≠cio da figura LOCAL
    double local_b = 0.;       # X Fim da figura LOCAL
    long long int local_n = 0; # Numero de mini trapezios LOCAL

    double area_relativa = 0.; # Area relativa ao intervalo
    double area_total = 0.;    # Area total

    MPI_Init(&argc, &argv);

    # Rank do processo
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

    # Quantos processos ent√£o sendo usados
    MPI_Comm_size(MPI_COMM_WORLD, &p);

    # Destribui o valor de n para todos os processos
    setSize(argc, argv, my_rank, &n);

    # O incremento e local_n ser√£o os mesmo para todos os processos
    inc = (xb - xa) / n;
    local_n = n / p;

    # O tamanho de cada intervalo de processo ser√° (local_n * inc)
    local_a = xa + my_rank * (local_n * inc);
    local_b = local_a + (local_n * inc);

    # Bloqueia o processo at√© todos chegarem nesse ponto
    MPI_Barrier(MPI_COMM_WORLD);

    area_relativa = trapezioIntegral(local_a, local_b, local_n);

    # Soma as integrais calculadas por cada processo
    MPI_Reduce(&area_relativa, &area_total, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

    if (my_rank == 0)
    {
        gettimeofday(&stop, 0);

        FILE *fp;
        char outputFilename[] = "./trapezio/tempo_mpi_trapezio.txt";

        fp = fopen(outputFilename, "a");
        if (fp == NULL)
        {
            fprintf(stderr, "Nao foi possivel abrir o arquivo %s!\n", outputFilename);
            exit(1);
        }

        fprintf(fp, "\tTempo: %1.2e \tResultado: %f\n", 
        ((double)(stop.tv_usec - start.tv_usec) / 1000000 + (double)(stop.tv_sec - start.tv_sec)),
        area_total);

        fclose(fp);
    }
    else
    { /* Nothing */ }

    MPI_Finalize();
}
```

## Resultados
### C√°lculo do Pi
Para esta an√°lise, ser√£o realizadas **5 execu√ß√µes** com tamanhos de problema 374.500.000, 550.000.000, 900.000.000 e 1.500.000.000 - definidos empiricamente de modo a atingir os limites m√≠nimos determinados pela [refer√™ncia](https://github.com/ozielalves/prog-paralela/tree/master/referencia) - em **3 quantidades de cores** (2, 4 e 8). Se espera que o comportamento de ambos os algoritmos quanto a aproxima√ß√£o do Pi seja parecido para um mesmo tamanho de problema quando se altera apenas o n√∫mero de cores, sendo o tempo de execu√ß√£o o √∫nico fator vari√°vel. Uma descri√ß√£o completa da m√°quina de testes pode ser encontrada no t√≥pico [Condi√ß√µes de Testes](#condi√ß√µes-de-testes).

#### Corretude

Para validar a corretude dos algoritmos implementados foi realizado um teste utilizando **4.550.000** como tamanho de problema para os dois c√≥digos:

![Alt Corretude - Pi Paralelo e Pi Serial](./data/pi_graphs/pi_terminal_print.PNG)

Como √© poss√≠vel perceber, ambos os c√≥digos conseguem aproximar de maneira correta o valor de pi, dado o n√∫mero de pontos solicitados.<br><br>
**Obs.:** Vale salientar que para este modelo de amostragem quanto maior o n√∫mero de pontos a serem definidos mais preciso ser√° o valor de pi retornado.


#### Gr√°ficos

##### Serial e Paralelo - Tempo x Tamanho do Problema

![Alt Serial e Paralelo - Tempo x Tamanho do Problema](./data/pi_graphs/serial_paralelo_tempo_por_tamanho_do_problema.PNG)

Atrav√©s do gr√°fico comparativo, √© poss√≠vel observar que o c√≥digo paralelo √© mais eficiente que o c√≥digo serial pois a reta relativa a este √∫ltimo apresenta um coefiente angular maior do que as relativas ao primeiro, o que indica que ao se aumentar o temanho de problema no c√≥digo serial o aumento em tempo de execu√ß√£o √© proporcionalmente maior que o que seria observado no c√≥digo paralelo. Vale salientar que as curvas referentes a 4 e 8 cores s√£o praticamente id√™nticas, isso ocorre devido aos limites da m√°quina de teste, fen√¥meno que ser√° mais bem explicado no item [Considera√ß√µes Finais](#consider√ß√µes-finais).

##### Tamanho do Problema - Tempo x Cores

![Alt Paralelo - Tempo x Cores](./data/pi_graphs/paralelo_tempo_por_cores.PNG)

A partir do gr√°fico apresentado, √© clara a influ√™ncia do n√∫mero de cores no tempo de execu√ß√£o. Note que, por exemplo, o tempo de execu√ß√£o para o problema de maior tamanho cai cerca de 45% ao se passar do c√≥digo serial para o c√≥digo paralelo ultilizando 2 cores. Novamente, verifica-se que o desempenho para 4 e 8 cores √© id√™ntico.


#### An√°lise de Speedup
√â poss√≠vel definir o speedup, quando da utiliza√ß√£o de n cores, como sendo o tempo de execu√ß√£o no c√≥digo serial dividido pelo tempo m√©dio de execu√ß√£o para n cores em um dado tamanho de problema. Dessa forma, o speedup representa um aumento m√©dio de velocidade na resolu√ß√£o dos problemas. Sabendo que o limite de cores/threads da m√°quina de testes √© 4, √© esperado que o espeedup m√©dio da execu√ß√£o dos probelmas para 4 e 8 cores seja aproximadamente id√™ntico.

### Speedup x N√∫mero de Cores Utilizados

![Alt Speedup x Cores](./data/pi_graphs/Speedup_pi.jpg)

((((((((((((((((((TODO

A tabela abaixo apresenta o speedup m√©dio por n√∫mero de cores ap√≥s 5 tentativas de execu√ß√£o dos 4 problemas descritos neste item.

| N√∫mero de Cores | 2 | 4 | 8 |
| --- | --- | ---| --- |
|**Speedup M√©dio**|1.80|2.47|2.44| 

#### An√°lise de Efici√™ncia
Atrav√©s do c√°culo do speedup, √© poss√≠vel obter a efici√™ncia do algoritmo quando submetido a execu√ß√£o com as diferentes quantidades de cores. Este c√°lculo pode ser realizado atrav√©s da divis√£o do speedup do algoritmo utilizando n cores pelos n cores utilizados.

### Efici√™cia x Tamanhos do Problema

![Alt Efici√™cia x Tamanhos do Problema](./data/pi_graphs/Eficiencia_pi.jpg)

((((((((((((TODO))))))))))))

Desse modo, ap√≥s o c√°culo da efici√™ncia, √© poss√≠vel definir o algoritmo analisado como **fracamente escal√°vel**, isto √©, quando o valor da efici√™ncia reduz conforme aumentamos o n√∫mero de cores utilizados. Apesar dos limites da m√°quina de testes, a efici√™ncia reduz de maneira consider√°vel se compararmos o passo no uso de 2 para 4 cores. A tabela abaixo apresenta a efici√™ncia m√©dia calculada atrav√©s dos valores de speedup anteriormente mencionados.

OBS.: Colocar mais processos gera mais comunica√ß√£o, o que implica em uma maior dist√¢ncia em rela√ßao a efici√™ncia linear

| N√∫mero de Cores | 2 | 4 | 8 |
| --- | --- | ---| --- |
|**Efici√™ncia M√©dia**|0.90|0.61|0.30| 

#### C√°lculo da Integral
Para esta an√°lise, ser√£o realizadas **5 execu√ß√µes** com tamanhos de problema 1.200.000.000, 2.400.000.000, 4.800.000.000 e 9.600.000.000, intervalo no eixo X de 0.0 a 30.0, e fun√ß√£o a ser integrada definida como `f(x) = pow(x, 2)` - ambos definidos empiricamente de modo a atingir os limites m√≠nimos determinados pela [refer√™ncia](https://github.com/ozielalves/prog-paralela/blob/master/referencia/Regras_do_trabalho_MPI_1.pdf) - em **3 quantidades de cores** (2, 4 e 8). Se espera que o comportamento de ambos os algoritmos quanto ao c√°lculo da integral seja id√™ntico para um mesmo tamanho de problema quando se altera apenas o n√∫mero de cores, sendo o tempo de execu√ß√£o o √∫nico fator vari√°vel. Uma descri√ß√£o completa da m√°quina de testes pode ser encontrada no t√≥pico [Condi√ß√µes de Testes](#condi√ß√µes-de-testes).

#### Corretude

Para validar a corretude dos algoritmos implementados foi realizado um teste utilizando **210.000** como tamanho de problema para os dois c√≥digos:

![Alt Corretude - Trapezio Paralelo e Trapezio Serial](./data/trapezio_graphs/trapezio_corretude.PNG)

Como √© poss√≠vel perceber nas impress√µes, ambos os c√≥digos conseguem aproximar de maneira correta o valor da integral de `f(x) = pow(x, 2)` de 0.0 at√© 30.0, dado o n√∫mero de trap√©zios solicitados.<br><br>
**Obs.:** Vale salientar que para este modelo de amostragem quanto maior o n√∫mero de trap√©zios mais precisa ser√° a integral da fun√ß√£o no intervalo selecionado.

#### Gr√°ficos

##### Serial e Paralelo - Tempo x Tamanho do Problema

![Alt Serial e Paralelo - Tempo x Tamanho do Problema](./data/trapezio_graphs/trap_velocidade_tamanho.jpg)

Atrav√©s do gr√°fico comparativo, √© poss√≠vel observar que o c√≥digo paralelo √© mais eficiente que o c√≥digo serial pois a reta relativa a este √∫ltimo apresenta um coefiente angular maior do que as relativas ao primeiro, o que indica que ao se aumentar o tamanho de problema no c√≥digo serial o aumento em tempo de execu√ß√£o √© proporcionalmente maior que o observado no c√≥digo paralelo. Note que a redu√ß√£o no tempo de execu√ß√£o do c√≥digo paralelo para o c√≥digo serial orcorre de maneira proporcional ao tamanho dos problemas. Vale salientar que as curvas referentes a 4 e 8 cores s√£o praticamente id√™nticas, isso ocorre devido aos limites da m√°quina de teste, fen√¥meno que ser√° mais bem explicado no item [Considera√ß√µes Finais](#consider√ß√µes-finais).

##### Execu√ß√£o do Problema - Tempo x Cores

![Alt Paralelo - Tempo x Cores](./data/trapezio_graphs/trap_velocidade_cores.jpg)

A partir do gr√°fico apresentado, √© clara a influ√™ncia do n√∫mero de cores no tempo de execu√ß√£o. Por exemplo, o tempo de execu√ß√£o para o problema de maior tamanho no c√≥digo serial cai para cerca de 8% ao se passar para o c√≥digo paralelo ultilizando 4 cores. Novamente, verifica-se que o desempenho para 4 e 8 cores √© id√™ntico.

#### An√°lise de Speedup
√â poss√≠vel definir o speedup, quando da utiliza√ß√£o de n cores, como sendo o tempo de execu√ß√£o no c√≥digo serial dividido pelo tempo m√©dio de execu√ß√£o para n cores em um dado tamanho de problema. Dessa forma, o speedup representa um aumento m√©dio de velocidade na resolu√ß√£o dos problemas.

### Speedup x N√∫mero de Cores Utilizados

![Alt Speedup x Cores](./data/trapezio_graphs/Speedup_trapezio.jpg)

((((((((((((((((((TODO

A tabela abaixo apresenta o speedup m√©dio por n√∫mero de cores ap√≥s 5 tentativas de execu√ß√£o dos 4 problemas descritos neste item.

| N√∫mero de Cores | 2 | 4 | 8 |
| --- | --- | ---| --- |
|**Speedup M√©dio**|2.85|3.26|3.41| 

#### An√°lise de Efici√™ncia
Atrav√©s do c√°culo do speedup, √© poss√≠vel obter a efici√™ncia do algoritmo quando submetido a execu√ß√£o com as diferentes quantidades de cores. Este c√°lculo pode ser realizado atrav√©s da divis√£o do speedup do algoritmo utilizando n cores pelos n cores utilizados. 

### Efici√™cia x Tamanhos do Problema

![Alt Efici√™cia x Tamanhos do Problema](./data/trapezio_graphs/Eficiencia_trapezio.jpg)

((((((((((((TODO))))))))))))

Desse modo, ap√≥s o c√°culo da efici√™ncia, √© poss√≠vel definir o algoritmo analisado como de **baixa escalabilidade**, isto √©, quando o valor da efici√™ncia reduz conforme aumentamos o n√∫mero de cores utilizados. A tabela abaixo apresenta a efici√™ncia m√©dia calculada atrav√©s dos valores de speedup anteriormente mencionados.

| N√∫mero de Cores | 2 | 4 | 8 |
| --- | --- | ---| --- |
|**Efici√™ncia M√©dia**|1.42|0.81|0.42|

## Conclus√£o

### Considera√ß√µes Finais

Devido aos limites da m√°quina de testes, o n√∫mero de cores pass√≠veis de utiliza√ß√£o √© restrito. Das an√°lises apresentadas, fica explicito que 4 cores √© o limite do dispositivo de maneira a ter um speedup relevante, apesar do processador integrar HyperThreading n√£o foi poss√≠vel estender o n√∫mero de cores utilizados para 8. Apesar disto, atrav√©s desta an√°lise foi poss√≠vel perceber que a paraleliza√ß√£o de c√≥digos seriais, ainda que simples, traz resultados bastante promissores no que diz respeito a efici√™ncia e velocidade. Al√©m disto, tamb√©m foi permitido constatar que o speedup √© ainda mais pronunciado para tamanhos maiores de problema. No entando, isto n√£o quer dizer necess√°riamente que o algoritmo tenha uma boa escalabiliade.

### Softwares utilizados
```bash
~$: g++ --version
g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

```bash
~$: python3 --version
Python 3.6.4
```

```bash
~$: grip --version
Grip 4.2.0 
```
